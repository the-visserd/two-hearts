{"cells":[{"cell_type":"markdown","metadata":{"id":"qIERnt9pC-yz"},"source":["# Questioning the Effect of Physiological Heartbeat Synchrony in Romantic Dyads. A Preregistered Deep Learning Analysis."]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1715,"status":"ok","timestamp":1649798541223,"user":{"displayName":"Thou Mightst","userId":"08013796147118011125"},"user_tz":-120},"id":"2raDtkxkC-y2","outputId":"79a29e05-38d2-47d4-b759-02f67aa4929d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# For Google Colab only:\n","from google.colab import drive\n","drive.mount('/content/drive/MyDrive/Masterarbeit/Code/two-hearts/')\n","google = \"/content/drive/MyDrive/Masterarbeit/Code/two-hearts/\"\n","\n","import sys\n","sys.path.append(google)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1649798541230,"user":{"displayName":"Thou Mightst","userId":"08013796147118011125"},"user_tz":-120},"id":"JXC8BydxC-zA","outputId":"bd176c14-76e5-48f3-f8e5-4ffb3da6359b"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow version: 2.3.0\n","Hotfix applied\n"]}],"source":["# Import libraries\n","import os\n","import datetime\n","\n","import IPython\n","import IPython.display\n","import random\n","import numpy as np\n","from numpy import array, hstack\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle as pkl\n","import pickle\n","from tensorflow.python.keras.layers import deserialize, serialize\n","from tensorflow.python.keras.saving import saving_utils\n","\n","import tensorflow.keras\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, BatchNormalization, multiply, concatenate, Flatten, Activation, dot\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import EarlyStopping\n","import pydot as pyd\n","from tensorflow.keras.utils import plot_model, model_to_dot\n","tensorflow.keras.utils.pydot = pyd\n","\n","from lists import list_str\n","\n","print(\"TensorFlow version:\",tensorflow.version.VERSION)\n","\n","# Hotfix for tensorflow < 2.6.0 to make keras models pickable\n","# from https://github.com/tensorflow/tensorflow/issues/34697#issuecomment-627193883\n","\n","def unpack(model, training_config, weights):\n","    restored_model = deserialize(model)\n","    if training_config is not None:\n","        restored_model.compile(\n","            **saving_utils.compile_args_from_training_config(\n","                training_config\n","            )\n","        )\n","    restored_model.set_weights(weights)\n","    return restored_model\n","\n","# Hotfix function\n","def make_keras_picklable():\n","\n","    def __reduce__(self):\n","        model_metadata = saving_utils.model_metadata(self)\n","        training_config = model_metadata.get(\"training_config\", None)\n","        model = serialize(self)\n","        weights = self.get_weights()\n","        return (unpack, (model, training_config, weights))\n","\n","    cls = Model\n","    cls.__reduce__ = __reduce__\n","\n","# Run the function\n","ver = tensorflow.version.VERSION\n","if float(ver[:-2]) < 2.6: \n","    make_keras_picklable()\n","    print(\"Hotfix applied\")"]},{"cell_type":"markdown","metadata":{"id":"iYhFodT5C-y_"},"source":["## Deep Learning"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sampling rate: 50\n","Time steps: 250 100\n","Conditions: ['sit', 'gaze', 'gaze_swap']\n","Participants: ['01', '02', '03', '04']\n"]}],"source":["# Set sampling rate\n","sampling_rate = 50\n","print(\"Sampling rate:\", sampling_rate)\n","\n","# Set number of time steps\n","n_steps_in, n_steps_out = 5*sampling_rate, 2*sampling_rate\n","print(\"Time steps:\", n_steps_in, n_steps_out)\n","\n","condition = [\"sit\",\"gaze\",\"gaze_swap\"]\n","print(\"Conditions:\", condition)\n","\n","print(\"Participants:\",list_str)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1649798541463,"user":{"displayName":"Thou Mightst","userId":"08013796147118011125"},"user_tz":-120},"id":"J5YnoKaSndWU"},"outputs":[],"source":["# Split a multivariate sequence into samples (modified from Brownlee 2018, p.156)\n","def split_sequences(sequences, n_steps_in, n_steps_out):\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        if i % (sampling_rate) == 0: # to remove redundancy in samples\n","            # find the end of this pattern\n","            end_ix = i + n_steps_in\n","            out_end_ix = end_ix + n_steps_out\n","            # check if we are beyond the dataset\n","            if out_end_ix > len(sequences):\n","                break\n","            # gather input and output parts of the pattern\n","            seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n","            X.append(seq_x)\n","            y.append(seq_y)\n","    return array(X), array(y)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Prepare sample data\n","def sample_preperation(condition): #google_colab = False\n","    # Load data\n","    data = np.load(\"data/data_\"+condition+\".npy\")\n","    print(\"Loaded data with shape\", data.shape,\"and type\",data.dtype)\n","\n","    # Create samples\n","    X_input_train = X_input_vali = X_input_test = np.empty((0, n_steps_in, 2))\n","    y_output_train = y_output_vali = y_output_test = np.empty((0, n_steps_out, 2))\n","\n","    idx = list(range(len(list_str)))[::2] # for all dyads\n","    # idx = [0] # for testing with 1 dyad only\n","\n","    for i in range(len(idx)):\n","        # define input sequence\n","        in_seq1 = data[idx[i]]\n","        in_seq2 = data[idx[i]+1]\n","        # convert to [rows, columns] structure\n","        in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n","        in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n","        # horizontally stack columns\n","        dataset = hstack((in_seq1, in_seq2))\n","        # covert into input/output\n","        X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n","        # data split\n","        num_train_samples = int(0.6 * len(X))\n","        num_val_samples = int(0.2 * len(X))\n","        num_test_samples = len(X) - num_train_samples - num_val_samples\n","        # append data for multiple dyads\n","        X_input_train = np.append(X_input_train,X[:num_train_samples],axis=0)\n","        y_output_train = np.append(y_output_train,y[:num_train_samples],axis=0)\n","        X_input_vali = np.append(X_input_vali,X[num_train_samples:(num_train_samples+num_val_samples)],axis=0)\n","        y_output_vali = np.append(y_output_vali,y[num_train_samples:(num_train_samples+num_val_samples)],axis=0)\n","        X_input_test = np.append(X_input_test,X[(num_train_samples+num_val_samples):],axis=0)\n","        y_output_test = np.append(y_output_test,y[(num_train_samples+num_val_samples):],axis=0)\n","    \n","    # Create dictionary\n","    samples = {\n","        \"X_input_train\": X_input_train,\n","        \"y_output_train\": y_output_train,\n","        \"X_input_vali\": X_input_vali,\n","        \"y_output_vali\": y_output_vali,\n","        \"X_input_test\": X_input_test,\n","        \"y_output_test\": y_output_test\n","    }\n","\n","    ## Plot data\n","    # fig = plt.figure(figsize=(6,1), dpi=96)\n","    # example = np.append(X_input_train[14,:,0], y_output_train[14,:,0])\n","    # example2 = np.append(X_input_train[14,:,1], y_output_train[14,:,1])\n","    # plt.plot(example)\n","    # plt.plot(example2)\n","\n","    print(\"Number of dyads:\", max(idx))\n","    print(\"num_train_samples per dyad:\", num_train_samples)\n","    print(\"num_val_samples per dyad:\", num_val_samples)\n","    print(\"num_test_samples per dyad:\", num_test_samples)\n","    print(\"Length of samples for each set:\", len(X_input_train), len(X_input_vali), len(X_input_test))\n","\n","    return samples"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1649798542473,"user":{"displayName":"Thou Mightst","userId":"08013796147118011125"},"user_tz":-120},"id":"jpbTCapSdNK7","outputId":"108e1fb0-d83f-418e-b012-a1605cc041f6"},"outputs":[],"source":["# Define simple seq2seq model \n","# Modified from Wieniawska 2020 \n","# (https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb)\n","\n","def lstm_decoder_encoder(samples, n_hidden = 100):\n","    # Input layer\n","    input_train = Input(shape=(samples[\"X_input_train\"].shape[1], samples[\"X_input_train\"].shape[2]))\n","    output_train = Input(shape=(samples[\"y_output_train\"].shape[1], samples[\"y_output_train\"].shape[2]))\n","    # print(input_train)\n","    # print(output_train)\n","\n","    # Encoder LSTM with state_h and state_c\n","    encoder_last_h1, encoder_last_h2, encoder_last_c = LSTM(\n","    n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, \n","    return_sequences=False, return_state=True)(input_train)\n","    # print(encoder_last_h1)\n","    # print(encoder_last_h2)\n","    # print(encoder_last_c)\n","\n","    # Batch normalisation to avoid gradient explosion\n","    encoder_last_h1 = BatchNormalization(momentum=0.6)(encoder_last_h1)\n","    encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)\n","\n","    # Decoder LSTM\n","    decoder = RepeatVector(output_train.shape[1])(encoder_last_h1)\n","    decoder = LSTM(n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, return_state=False, return_sequences=True)(\n","        decoder, initial_state=[encoder_last_h1, encoder_last_c])\n","    # print(decoder)\n","\n","    # Dense layer with repeated weights\n","    out = TimeDistributed(Dense(output_train.shape[2]))(decoder)\n","    # print(out)\n","\n","    # Compile model\n","    model = Model(inputs=input_train, outputs=out)\n","    opt = Adam(learning_rate=0.001, clipnorm=1)\n","    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n","    model.summary()\n","\n","    return model"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Define seq2seq model with Loung attention\n","# Modified from Wieniawska 2020 \n","# (https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb)\n","\n","def lstm_decoder_encoder_loung_attention(samples, n_hidden = 100):\n","    # Input layer\n","    input_train = Input(shape=(samples[\"X_input_train.shape[1]\"], samples[\"X_input_train.shape[2]\"]))\n","    output_train = Input(shape=(samples[\"y_output_train.shape[1]\"], samples[\"y_output_train.shape[2]\"]))\n","\n","    # Encoder LSTM\n","    encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n","        n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, \n","        return_state=True, return_sequences=True)(input_train)\n","    # print(encoder_stack_h)\n","    # print(encoder_last_h)\n","    # print(encoder_last_c)\n","\n","    # Batch normalisation to avoid gradient explosion\n","    encoder_last_h = BatchNormalization(momentum=0.6)(encoder_last_h)\n","    encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)\n","\n","    # Decoder LSTM\n","    decoder_input = RepeatVector(output_train.shape[1])(encoder_last_h)\n","    # print(decoder_input)\n","\n","    decoder_stack_h = LSTM(n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2,\n","    return_state=False, return_sequences=True)(\n","    decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n","    # print(decoder_stack_h)\n","\n","    # Attention layer\n","    attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n","    attention = Activation('softmax')(attention)\n","    # print(attention)\n","\n","    # Calculate context vector with batch normalisation\n","    context = dot([attention, encoder_stack_h], axes=[2,1])\n","    context = BatchNormalization(momentum=0.6)(context)\n","    # print(context)\n","\n","    # Combine context vector with stacked hidden states of decoder for input to the last dense layer\n","    decoder_combined_context = concatenate([context, decoder_stack_h])\n","    # print(decoder_combined_context)\n","\n","    # Dense layer with repeated weights\n","    out = TimeDistributed(Dense(output_train.shape[2]))(decoder_combined_context)\n","    # print(out)\n","\n","    # Compile model\n","    model = Model(inputs=input_train, outputs=out)\n","    opt = Adam(learning_rate=0.001, clipnorm=1)\n","    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n","    model.summary()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1649798542760,"user":{"displayName":"Thou Mightst","userId":"08013796147118011125"},"user_tz":-120},"id":"LIgKaIn-dNK8","outputId":"2275debe-652d-4f80-c25c-09922b4b7ddb"},"outputs":[],"source":["# Fit model\n","def fit_model(model, samples):\n","    epc = 300\n","    es = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True)\n","    history = model.fit(samples[\"X_input_train\"], samples[\"y_output_train\"],  validation_data=(samples[\"X_input_vali\"],samples[\"y_output_vali\"]), \n","                        epochs=epc, verbose=1, callbacks=[es], \n","                        batch_size=64, shuffle=False)\n","    # model.save(\"model_forecasting_seq2seq.h5\")\n","\n","    return model, history"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     [(None, 100), (None, 41200       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 100)          400         lstm[0][0]                       \n","__________________________________________________________________________________________________\n","repeat_vector (RepeatVector)    (None, 100, 100)     0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 100)          400         lstm[0][2]                       \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 100, 100)     80400       repeat_vector[0][0]              \n","                                                                 batch_normalization[0][0]        \n","                                                                 batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 100, 2)       202         lstm_1[0][0]                     \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 7s 1s/step - loss: 0.2578 - mae: 0.4003 - val_loss: 0.1260 - val_mae: 0.3183\n","Epoch 2/3\n","6/6 [==============================] - 5s 863ms/step - loss: 0.1469 - mae: 0.3315 - val_loss: 0.1267 - val_mae: 0.3182\n","Epoch 3/3\n","6/6 [==============================] - 5s 842ms/step - loss: 0.1403 - mae: 0.3250 - val_loss: 0.1236 - val_mae: 0.3137\n","2/2 [==============================] - 0s 91ms/step - loss: 0.1237 - mae: 0.3136\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, 100), (None, 41200       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 100)          400         lstm_2[0][0]                     \n","__________________________________________________________________________________________________\n","repeat_vector_1 (RepeatVector)  (None, 100, 100)     0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 100)          400         lstm_2[0][2]                     \n","__________________________________________________________________________________________________\n","lstm_3 (LSTM)                   (None, 100, 100)     80400       repeat_vector_1[0][0]            \n","                                                                 batch_normalization_2[0][0]      \n","                                                                 batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 100, 2)       202         lstm_3[0][0]                     \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 7s 1s/step - loss: 0.2360 - mae: 0.3903 - val_loss: 0.1248 - val_mae: 0.3176\n","Epoch 2/3\n","6/6 [==============================] - 6s 965ms/step - loss: 0.1409 - mae: 0.3260 - val_loss: 0.1233 - val_mae: 0.3135\n","Epoch 3/3\n","6/6 [==============================] - 5s 810ms/step - loss: 0.1322 - mae: 0.3187 - val_loss: 0.1213 - val_mae: 0.3101\n","2/2 [==============================] - 0s 71ms/step - loss: 0.1217 - mae: 0.3111\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_4 (LSTM)                   [(None, 100), (None, 41200       input_5[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 100)          400         lstm_4[0][0]                     \n","__________________________________________________________________________________________________\n","repeat_vector_2 (RepeatVector)  (None, 100, 100)     0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 100)          400         lstm_4[0][2]                     \n","__________________________________________________________________________________________________\n","lstm_5 (LSTM)                   (None, 100, 100)     80400       repeat_vector_2[0][0]            \n","                                                                 batch_normalization_4[0][0]      \n","                                                                 batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 100, 2)       202         lstm_5[0][0]                     \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 7s 1s/step - loss: 0.4686 - mae: 0.5109 - val_loss: 0.1264 - val_mae: 0.3190\n","Epoch 2/3\n","6/6 [==============================] - 6s 925ms/step - loss: 0.1556 - mae: 0.3377 - val_loss: 0.1330 - val_mae: 0.3223\n","Epoch 3/3\n","6/6 [==============================] - 5s 753ms/step - loss: 0.1433 - mae: 0.3280 - val_loss: 0.1256 - val_mae: 0.3151\n","2/2 [==============================] - 0s 58ms/step - loss: 0.1276 - mae: 0.3163\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_7\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_6 (LSTM)                   [(None, 100), (None, 41200       input_7[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 100)          400         lstm_6[0][0]                     \n","__________________________________________________________________________________________________\n","repeat_vector_3 (RepeatVector)  (None, 100, 100)     0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 100)          400         lstm_6[0][2]                     \n","__________________________________________________________________________________________________\n","lstm_7 (LSTM)                   (None, 100, 100)     80400       repeat_vector_3[0][0]            \n","                                                                 batch_normalization_6[0][0]      \n","                                                                 batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 100, 2)       202         lstm_7[0][0]                     \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 6s 1s/step - loss: 0.8386 - mae: 0.4926 - val_loss: 0.1280 - val_mae: 0.3210\n","Epoch 2/3\n","6/6 [==============================] - 5s 802ms/step - loss: 0.1561 - mae: 0.3389 - val_loss: 0.1339 - val_mae: 0.3249\n","Epoch 3/3\n","6/6 [==============================] - 5s 774ms/step - loss: 0.1430 - mae: 0.3299 - val_loss: 0.1279 - val_mae: 0.3170\n","2/2 [==============================] - 0s 63ms/step - loss: 0.1278 - mae: 0.3170\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_9\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_9 (InputLayer)            [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_8 (LSTM)                   [(None, 100), (None, 41200       input_9[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 100)          400         lstm_8[0][0]                     \n","__________________________________________________________________________________________________\n","repeat_vector_4 (RepeatVector)  (None, 100, 100)     0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 100)          400         lstm_8[0][2]                     \n","__________________________________________________________________________________________________\n","lstm_9 (LSTM)                   (None, 100, 100)     80400       repeat_vector_4[0][0]            \n","                                                                 batch_normalization_8[0][0]      \n","                                                                 batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","time_distributed_4 (TimeDistrib (None, 100, 2)       202         lstm_9[0][0]                     \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 7s 1s/step - loss: 0.2413 - mae: 0.3959 - val_loss: 0.1262 - val_mae: 0.3185\n","Epoch 2/3\n","6/6 [==============================] - 5s 828ms/step - loss: 0.1466 - mae: 0.3314 - val_loss: 0.1293 - val_mae: 0.3192\n","Epoch 3/3\n","6/6 [==============================] - 6s 958ms/step - loss: 0.1346 - mae: 0.3215 - val_loss: 0.1290 - val_mae: 0.3181\n","2/2 [==============================] - 0s 84ms/step - loss: 0.1277 - mae: 0.3171\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_11\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_11 (InputLayer)           [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_10 (LSTM)                  [(None, 100), (None, 41200       input_11[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 100)          400         lstm_10[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_5 (RepeatVector)  (None, 100, 100)     0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 100)          400         lstm_10[0][2]                    \n","__________________________________________________________________________________________________\n","lstm_11 (LSTM)                  (None, 100, 100)     80400       repeat_vector_5[0][0]            \n","                                                                 batch_normalization_10[0][0]     \n","                                                                 batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","time_distributed_5 (TimeDistrib (None, 100, 2)       202         lstm_11[0][0]                    \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 9s 1s/step - loss: 0.2233 - mae: 0.3871 - val_loss: 0.1250 - val_mae: 0.3172\n","Epoch 2/3\n","6/6 [==============================] - 9s 2s/step - loss: 0.1540 - mae: 0.3353 - val_loss: 0.1353 - val_mae: 0.3224\n","Epoch 3/3\n","6/6 [==============================] - 9s 2s/step - loss: 0.1403 - mae: 0.3253 - val_loss: 0.1284 - val_mae: 0.3165\n","2/2 [==============================] - 0s 141ms/step - loss: 0.1286 - mae: 0.3174\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_13\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_13 (InputLayer)           [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_12 (LSTM)                  [(None, 100), (None, 41200       input_13[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 100)          400         lstm_12[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_6 (RepeatVector)  (None, 100, 100)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 100)          400         lstm_12[0][2]                    \n","__________________________________________________________________________________________________\n","lstm_13 (LSTM)                  (None, 100, 100)     80400       repeat_vector_6[0][0]            \n","                                                                 batch_normalization_12[0][0]     \n","                                                                 batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","time_distributed_6 (TimeDistrib (None, 100, 2)       202         lstm_13[0][0]                    \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 12s 2s/step - loss: 0.4922 - mae: 0.3981 - val_loss: 0.1275 - val_mae: 0.3203\n","Epoch 2/3\n","6/6 [==============================] - 10s 2s/step - loss: 0.1536 - mae: 0.3369 - val_loss: 0.1254 - val_mae: 0.3159\n","Epoch 3/3\n","6/6 [==============================] - 9s 1s/step - loss: 0.1395 - mae: 0.3245 - val_loss: 0.1249 - val_mae: 0.3125\n","2/2 [==============================] - 0s 81ms/step - loss: 0.1244 - mae: 0.3120\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_15\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_15 (InputLayer)           [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_14 (LSTM)                  [(None, 100), (None, 41200       input_15[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 100)          400         lstm_14[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_7 (RepeatVector)  (None, 100, 100)     0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 100)          400         lstm_14[0][2]                    \n","__________________________________________________________________________________________________\n","lstm_15 (LSTM)                  (None, 100, 100)     80400       repeat_vector_7[0][0]            \n","                                                                 batch_normalization_14[0][0]     \n","                                                                 batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","time_distributed_7 (TimeDistrib (None, 100, 2)       202         lstm_15[0][0]                    \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 7s 1s/step - loss: 0.2474 - mae: 0.3951 - val_loss: 0.1280 - val_mae: 0.3207\n","Epoch 2/3\n","6/6 [==============================] - 6s 937ms/step - loss: 0.1606 - mae: 0.3423 - val_loss: 0.1312 - val_mae: 0.3227\n","Epoch 3/3\n","6/6 [==============================] - 6s 936ms/step - loss: 0.1416 - mae: 0.3281 - val_loss: 0.1285 - val_mae: 0.3170\n","2/2 [==============================] - 0s 85ms/step - loss: 0.1298 - mae: 0.3187\n","Loaded data with shape (4, 14800) and type float32\n","Number of dyads: 2\n","num_train_samples per dyad: 174\n","num_val_samples per dyad: 58\n","num_test_samples per dyad: 58\n","Length of samples for each set: 348 116 116\n","Model: \"functional_17\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_17 (InputLayer)           [(None, 250, 2)]     0                                            \n","__________________________________________________________________________________________________\n","lstm_16 (LSTM)                  [(None, 100), (None, 41200       input_17[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 100)          400         lstm_16[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_8 (RepeatVector)  (None, 100, 100)     0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 100)          400         lstm_16[0][2]                    \n","__________________________________________________________________________________________________\n","lstm_17 (LSTM)                  (None, 100, 100)     80400       repeat_vector_8[0][0]            \n","                                                                 batch_normalization_16[0][0]     \n","                                                                 batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","time_distributed_8 (TimeDistrib (None, 100, 2)       202         lstm_17[0][0]                    \n","==================================================================================================\n","Total params: 122,602\n","Trainable params: 122,202\n","Non-trainable params: 400\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","6/6 [==============================] - 7s 1s/step - loss: 0.2427 - mae: 0.4012 - val_loss: 0.1274 - val_mae: 0.3193\n","Epoch 2/3\n","6/6 [==============================] - 6s 962ms/step - loss: 0.1506 - mae: 0.3333 - val_loss: 0.1256 - val_mae: 0.3159\n","Epoch 3/3\n","6/6 [==============================] - 6s 987ms/step - loss: 0.1365 - mae: 0.3226 - val_loss: 0.1273 - val_mae: 0.3154\n","2/2 [==============================] - 0s 98ms/step - loss: 0.1280 - mae: 0.3163\n"]}],"source":["# Execute over all conditions for n trials\n","trial = [\"01\",\"02\",\"03\"]\n","samples_all = {}\n","model_all = {}\n","history_all = {}\n","results_all = {}\n","data_all = {}\n","for x in range(len(trial)): data_all[trial[x]] = {} # prepare nested dictionary\n","\n","for j in range(len(trial)):\n","    for i in range(len(condition)):\n","        samples = sample_preperation(condition[i])\n","        samples_all[condition[i]] = samples\n","        model = lstm_decoder_encoder(samples)\n","        # model = lstm_decoder_encoder_loung_attention(samples)\n","        model, history = fit_model(model, samples)\n","        model_all[condition[i]] = model\n","        history_all[condition[i]] = history\n","        results = model.evaluate(samples[\"X_input_test\"], samples[\"y_output_test\"], batch_size=64)\n","        results_all[condition[i]] = results\n","    data_all[trial[j]][\"samples_all\"] = samples_all\n","    data_all[trial[j]][\"model_all\"] = model_all\n","    data_all[trial[j]][\"history_all\"] = history_all\n","    data_all[trial[j]][\"results_all\"] = results_all\n","\n","data_all_file = open(\"data_all.pkl\",\"wb\")\n","pkl.dump(data_all,data_all_file)\n","data_all_file.close()\n"]}],"metadata":{"accelerator":"TPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"02_forecast.ipynb","provenance":[{"file_id":"https://github.com/thevizzerd/two-hearts/blob/main/02_2D_two-hearts.ipynb","timestamp":1644170862158}]},"interpreter":{"hash":"140e3961e635afead8b973c7c4d4b2276a9d6240af442eb4cba65cde2b5b46ba"},"kernelspec":{"display_name":"Python 3.8.12 64-bit ('two-hearts': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
