{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIERnt9pC-yz"
      },
      "source": [
        "# Questioning the Effect of Physiological Heartbeat Synchrony in Romantic Dyads. A Preregistered Deep Learning Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4yMu0MJkcft",
        "outputId": "c0958329-058a-4df5-e17d-afe3ea8cd772"
      },
      "outputs": [],
      "source": [
        "# For Google Colab / local machine\n",
        "import tensorflow.keras\n",
        "ver = tensorflow.version.VERSION\n",
        "\n",
        "if float(ver[:3]) > 2.7:\n",
        "  print(\"Latest TensorFlow version detected -> Prepare Google Colab usage\\n\")\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  colab_path = \"/content/drive/MyDrive/Masterarbeit/Code/two-hearts/\"\n",
        "  import sys\n",
        "  sys.path.append(colab_path)\n",
        "else:\n",
        "  colab_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXC8BydxC-zA",
        "outputId": "de3bf7bc-bdb9-4c34-c447-7cfa2a32eb48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.3.0\n",
            "Hotfix applied\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from numpy import array, hstack\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "import pickle\n",
        "from tensorflow.python.keras.layers import deserialize, serialize\n",
        "from tensorflow.python.keras.saving import saving_utils\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, BatchNormalization, multiply, concatenate, Flatten, Activation, dot\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pydot as pyd\n",
        "from tensorflow.keras.utils import plot_model, model_to_dot\n",
        "tensorflow.keras.utils.pydot = pyd\n",
        "\n",
        "from lists import list_str\n",
        "\n",
        "print(\"TensorFlow version:\",tensorflow.version.VERSION)\n",
        "\n",
        "# Hotfix for tensorflow < 2.6.0 to make keras models pickable\n",
        "# from https://github.com/tensorflow/tensorflow/issues/34697#issuecomment-627193883\n",
        "\n",
        "def unpack(model, training_config, weights):\n",
        "    restored_model = deserialize(model)\n",
        "    if training_config is not None:\n",
        "        restored_model.compile(\n",
        "            **saving_utils.compile_args_from_training_config(\n",
        "                training_config\n",
        "            )\n",
        "        )\n",
        "    restored_model.set_weights(weights)\n",
        "    return restored_model\n",
        "\n",
        "# Hotfix function\n",
        "def make_keras_picklable():\n",
        "\n",
        "    def __reduce__(self):\n",
        "        model_metadata = saving_utils.model_metadata(self)\n",
        "        training_config = model_metadata.get(\"training_config\", None)\n",
        "        model = serialize(self)\n",
        "        weights = self.get_weights()\n",
        "        return (unpack, (model, training_config, weights))\n",
        "\n",
        "    cls = Model\n",
        "    cls.__reduce__ = __reduce__\n",
        "\n",
        "# Run the function\n",
        "if float(ver[:3]) < 2.6: \n",
        "    make_keras_picklable()\n",
        "    print(\"Hotfix applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYhFodT5C-y_"
      },
      "source": [
        "## Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT9ANStQiIMB",
        "outputId": "84800a00-e6da-4b32-d897-c49d7186d206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling rate: 50\n",
            "Time steps: 250 100\n",
            "Conditions: ['sit', 'gaze', 'gaze_swap']\n",
            "Participants: ['01', '02', '03', '04']\n"
          ]
        }
      ],
      "source": [
        "# Set sampling rate\n",
        "sampling_rate = 50\n",
        "print(\"Sampling rate:\", sampling_rate)\n",
        "\n",
        "# Set number of time steps\n",
        "n_steps_in, n_steps_out = 5*sampling_rate, 2*sampling_rate\n",
        "print(\"Time steps:\", n_steps_in, n_steps_out)\n",
        "\n",
        "condition = [\"sit\",\"gaze\",\"gaze_swap\"]\n",
        "print(\"Conditions:\", condition)\n",
        "\n",
        "print(\"Participants:\",list_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_a94i_w_iIME"
      },
      "outputs": [],
      "source": [
        "# Prepare sample data\n",
        "def sample_preperation(condition):# Split a multivariate sequence into samples (modified from Brownlee 2018, p.156)\n",
        "\n",
        "    # Load data\n",
        "    data = np.load(colab_path+\"data/data_\"+condition+\".npy\") # with colab_path\n",
        "    print(\"Loaded data with shape\", data.shape,\"and type\",data.dtype)\n",
        "\n",
        "    # Create samples\n",
        "    X_input_train = X_input_vali = X_input_test = np.empty((0, n_steps_in, 2))\n",
        "    y_output_train = y_output_vali = y_output_test = np.empty((0, n_steps_out, 2))\n",
        "\n",
        "    idx = list(range(len(list_str)))[::2] # for all dyads\n",
        "    # idx = [0] # for testing with 1 dyad only\n",
        "\n",
        "    # Split a multivariate sequence into samples (modified from Brownlee 2018, p.156)\n",
        "    def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "        X, y = list(), list()\n",
        "        for i in range(len(sequences)):\n",
        "            if i % (sampling_rate) == 0: # to remove redundancy in samples\n",
        "                # find the end of this pattern\n",
        "                end_ix = i + n_steps_in\n",
        "                out_end_ix = end_ix + n_steps_out\n",
        "                # check if we are beyond the dataset\n",
        "                if out_end_ix > len(sequences):\n",
        "                    break\n",
        "                # gather input and output parts of the pattern\n",
        "                seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
        "                X.append(seq_x)\n",
        "                y.append(seq_y)\n",
        "        return array(X), array(y)\n",
        "\n",
        "    for i in range(len(idx)):\n",
        "        # define input sequence\n",
        "        in_seq1 = data[idx[i]]\n",
        "        in_seq2 = data[idx[i]+1]\n",
        "        # convert to [rows, columns] structure\n",
        "        in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "        in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "        # horizontally stack columns\n",
        "        dataset = hstack((in_seq1, in_seq2))\n",
        "        # covert into input/output\n",
        "        X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
        "        # data split\n",
        "        num_train_samples = int(0.6 * len(X))\n",
        "        num_val_samples = int(0.2 * len(X))\n",
        "        num_test_samples = len(X) - num_train_samples - num_val_samples\n",
        "        # append data for multiple dyads\n",
        "        X_input_train = np.append(X_input_train,X[:num_train_samples],axis=0)\n",
        "        y_output_train = np.append(y_output_train,y[:num_train_samples],axis=0)\n",
        "        X_input_vali = np.append(X_input_vali,X[num_train_samples:(num_train_samples+num_val_samples)],axis=0)\n",
        "        y_output_vali = np.append(y_output_vali,y[num_train_samples:(num_train_samples+num_val_samples)],axis=0)\n",
        "        X_input_test = np.append(X_input_test,X[(num_train_samples+num_val_samples):],axis=0)\n",
        "        y_output_test = np.append(y_output_test,y[(num_train_samples+num_val_samples):],axis=0)\n",
        "    \n",
        "    # Create dictionary\n",
        "    samples = {\n",
        "        \"X_input_train\": X_input_train,\n",
        "        \"y_output_train\": y_output_train,\n",
        "        \"X_input_vali\": X_input_vali,\n",
        "        \"y_output_vali\": y_output_vali,\n",
        "        \"X_input_test\": X_input_test,\n",
        "        \"y_output_test\": y_output_test\n",
        "    }\n",
        "\n",
        "    ## Plot data\n",
        "    # fig = plt.figure(figsize=(6,1), dpi=96)\n",
        "    # example = np.append(X_input_train[14,:,0], y_output_train[14,:,0])\n",
        "    # example2 = np.append(X_input_train[14,:,1], y_output_train[14,:,1])\n",
        "    # plt.plot(example)\n",
        "    # plt.plot(example2)\n",
        "\n",
        "    print(\"Number of dyads:\", max(idx))\n",
        "    print(\"num_train_samples per dyad:\", num_train_samples)\n",
        "    print(\"num_val_samples per dyad:\", num_val_samples)\n",
        "    print(\"num_test_samples per dyad:\", num_test_samples)\n",
        "    print(\"Length of samples for each set:\", len(X_input_train), len(X_input_vali), len(X_input_test))\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jpbTCapSdNK7"
      },
      "outputs": [],
      "source": [
        "# Define simple seq2seq model \n",
        "# Modified from Wieniawska 2020 \n",
        "# (https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb)\n",
        "\n",
        "def lstm_decoder_encoder(samples, n_hidden = 100):\n",
        "    # Input layer\n",
        "    input_train = Input(shape=(samples[\"X_input_train\"].shape[1], samples[\"X_input_train\"].shape[2]))\n",
        "    output_train = Input(shape=(samples[\"y_output_train\"].shape[1], samples[\"y_output_train\"].shape[2]))\n",
        "    # print(input_train)\n",
        "    # print(output_train)\n",
        "\n",
        "    # Encoder LSTM with state_h and state_c\n",
        "    encoder_last_h1, encoder_last_h2, encoder_last_c = LSTM(\n",
        "    n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, \n",
        "    return_sequences=False, return_state=True)(input_train)\n",
        "    # print(encoder_last_h1)\n",
        "    # print(encoder_last_h2)\n",
        "    # print(encoder_last_c)\n",
        "\n",
        "    # Batch normalisation to avoid gradient explosion\n",
        "    encoder_last_h1 = BatchNormalization(momentum=0.6)(encoder_last_h1)\n",
        "    encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)\n",
        "\n",
        "    # Decoder LSTM\n",
        "    decoder = RepeatVector(output_train.shape[1])(encoder_last_h1)\n",
        "    decoder = LSTM(n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, return_state=False, return_sequences=True)(\n",
        "        decoder, initial_state=[encoder_last_h1, encoder_last_c])\n",
        "    # print(decoder)\n",
        "\n",
        "    # Dense layer with repeated weights\n",
        "    out = TimeDistributed(Dense(output_train.shape[2]))(decoder)\n",
        "    # print(out)\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=input_train, outputs=out)\n",
        "    opt = Adam(learning_rate=0.001, clipnorm=1)\n",
        "    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
        "    model.summary()\n",
        "    print(model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IVg46Gb1iIMI"
      },
      "outputs": [],
      "source": [
        "# Define seq2seq model with Loung attention\n",
        "# Modified from Wieniawska 2020 \n",
        "# (https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb)\n",
        "\n",
        "def lstm_decoder_encoder_loung_attention(samples, n_hidden = 100):\n",
        "    # Input layer\n",
        "    input_train = Input(shape=(samples[\"X_input_train.shape[1]\"], samples[\"X_input_train.shape[2]\"]))\n",
        "    output_train = Input(shape=(samples[\"y_output_train.shape[1]\"], samples[\"y_output_train.shape[2]\"]))\n",
        "\n",
        "    # Encoder LSTM\n",
        "    encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n",
        "        n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, \n",
        "        return_state=True, return_sequences=True)(input_train)\n",
        "    # print(encoder_stack_h)\n",
        "    # print(encoder_last_h)\n",
        "    # print(encoder_last_c)\n",
        "\n",
        "    # Batch normalisation to avoid gradient explosion\n",
        "    encoder_last_h = BatchNormalization(momentum=0.6)(encoder_last_h)\n",
        "    encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)\n",
        "\n",
        "    # Decoder LSTM\n",
        "    decoder_input = RepeatVector(output_train.shape[1])(encoder_last_h)\n",
        "    # print(decoder_input)\n",
        "\n",
        "    decoder_stack_h = LSTM(n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2,\n",
        "    return_state=False, return_sequences=True)(\n",
        "    decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
        "    # print(decoder_stack_h)\n",
        "\n",
        "    # Attention layer\n",
        "    attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
        "    attention = Activation('softmax')(attention)\n",
        "    # print(attention)\n",
        "\n",
        "    # Calculate context vector with batch normalisation\n",
        "    context = dot([attention, encoder_stack_h], axes=[2,1])\n",
        "    context = BatchNormalization(momentum=0.6)(context)\n",
        "    # print(context)\n",
        "\n",
        "    # Combine context vector with stacked hidden states of decoder for input to the last dense layer\n",
        "    decoder_combined_context = concatenate([context, decoder_stack_h])\n",
        "    # print(decoder_combined_context)\n",
        "\n",
        "    # Dense layer with repeated weights\n",
        "    out = TimeDistributed(Dense(output_train.shape[2]))(decoder_combined_context)\n",
        "    # print(out)\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=input_train, outputs=out)\n",
        "    opt = Adam(learning_rate=0.001, clipnorm=1)\n",
        "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
        "    model.summary()\n",
        "    print(model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LIgKaIn-dNK8"
      },
      "outputs": [],
      "source": [
        "# Fit model\n",
        "def fit_model(model, samples):\n",
        "    epc = 300\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True)\n",
        "    history = model.fit(samples[\"X_input_train\"], samples[\"y_output_train\"],  validation_data=(samples[\"X_input_vali\"],samples[\"y_output_vali\"]), \n",
        "                        epochs=epc, verbose=1, callbacks=[es], \n",
        "                        batch_size=64, shuffle=False)\n",
        "    # model.save(\"model_forecasting_seq2seq.h5\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data with shape (4, 14800) and type float32\n",
            "Number of dyads: 2\n",
            "num_train_samples per dyad: 174\n",
            "num_val_samples per dyad: 58\n",
            "num_test_samples per dyad: 58\n",
            "Length of samples for each set: 348 116 116\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 250, 2)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 100), (None, 41200       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 100)          400         lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 100, 100)     0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 100)          400         lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 100, 100)     80400       repeat_vector[0][0]              \n",
            "                                                                 batch_normalization[0][0]        \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 100, 2)       202         lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 122,602\n",
            "Trainable params: 122,202\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x0000028C8182C5E0>\n",
            "6/6 [==============================] - 6s 926ms/step - loss: 0.2691 - mae: 0.3867 - val_loss: 0.1245 - val_mae: 0.3169\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.1247 - mae: 0.3171\n",
            "Loaded data with shape (4, 14800) and type float32\n",
            "Number of dyads: 2\n",
            "num_train_samples per dyad: 174\n",
            "num_val_samples per dyad: 58\n",
            "num_test_samples per dyad: 58\n",
            "Length of samples for each set: 348 116 116\n",
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 250, 2)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 100), (None, 41200       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 100)          400         lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 100, 100)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 100)          400         lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 100, 100)     80400       repeat_vector_1[0][0]            \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 100, 2)       202         lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 122,602\n",
            "Trainable params: 122,202\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x0000028C938203A0>\n",
            "6/6 [==============================] - 6s 967ms/step - loss: 0.2673 - mae: 0.4098 - val_loss: 0.1261 - val_mae: 0.3187\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.1261 - mae: 0.3187\n",
            "Loaded data with shape (4, 14800) and type float32\n",
            "Number of dyads: 2\n",
            "num_train_samples per dyad: 174\n",
            "num_val_samples per dyad: 58\n",
            "num_test_samples per dyad: 58\n",
            "Length of samples for each set: 348 116 116\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 250, 2)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 100), (None, 41200       input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 100)          400         lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_2 (RepeatVector)  (None, 100, 100)     0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 100)          400         lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 100, 100)     80400       repeat_vector_2[0][0]            \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 100, 2)       202         lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 122,602\n",
            "Trainable params: 122,202\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x0000028CA367CA90>\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.5384 - mae: 0.5007 - val_loss: 0.1281 - val_mae: 0.3205\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 0.1283 - mae: 0.3206\n",
            "Loaded data with shape (4, 14800) and type float32\n",
            "Number of dyads: 2\n",
            "num_train_samples per dyad: 174\n",
            "num_val_samples per dyad: 58\n",
            "num_test_samples per dyad: 58\n",
            "Length of samples for each set: 348 116 116\n",
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 250, 2)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(None, 100), (None, 41200       input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 100)          400         lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_3 (RepeatVector)  (None, 100, 100)     0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 100)          400         lstm_6[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 100, 100)     80400       repeat_vector_3[0][0]            \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 100, 2)       202         lstm_7[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 122,602\n",
            "Trainable params: 122,202\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x0000028CAEE713A0>\n",
            "6/6 [==============================] - 12s 2s/step - loss: 0.2971 - mae: 0.4205 - val_loss: 0.1279 - val_mae: 0.3210\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.1283 - mae: 0.3215\n",
            "Loaded data with shape (4, 14800) and type float32\n",
            "Number of dyads: 2\n",
            "num_train_samples per dyad: 174\n",
            "num_val_samples per dyad: 58\n",
            "num_test_samples per dyad: 58\n",
            "Length of samples for each set: 348 116 116\n",
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 250, 2)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   [(None, 100), (None, 41200       input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 100)          400         lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_4 (RepeatVector)  (None, 100, 100)     0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 100)          400         lstm_8[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 100, 100)     80400       repeat_vector_4[0][0]            \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 100, 2)       202         lstm_9[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 122,602\n",
            "Trainable params: 122,202\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x0000028CBADDDBE0>\n",
            "6/6 [==============================] - 11s 2s/step - loss: 0.3223 - mae: 0.4255 - val_loss: 0.1310 - val_mae: 0.3230\n",
            "2/2 [==============================] - 0s 86ms/step - loss: 0.1310 - mae: 0.3229\n",
            "Loaded data with shape (4, 14800) and type float32\n",
            "Number of dyads: 2\n",
            "num_train_samples per dyad: 174\n",
            "num_val_samples per dyad: 58\n",
            "num_test_samples per dyad: 58\n",
            "Length of samples for each set: 348 116 116\n",
            "Model: \"functional_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 250, 2)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  [(None, 100), (None, 41200       input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 100)          400         lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_5 (RepeatVector)  (None, 100, 100)     0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 100)          400         lstm_10[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 100, 100)     80400       repeat_vector_5[0][0]            \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_5 (TimeDistrib (None, 100, 2)       202         lstm_11[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 122,602\n",
            "Trainable params: 122,202\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x0000028CBECD5730>\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.2453 - mae: 0.3924 - val_loss: 0.1263 - val_mae: 0.3189\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 0.1264 - mae: 0.3192\n"
          ]
        }
      ],
      "source": [
        "# Execute over all conditions for n trials\n",
        "trial = [\"01\",\"02\",\"03\"]\n",
        "samples_all = {}\n",
        "model_all = {}\n",
        "history_all = {}\n",
        "results_all = {}\n",
        "data_all = {}\n",
        "for x in range(len(trial)): data_all[trial[x]] = {} # prepare nested dictionary\n",
        "\n",
        "for j in range(len(trial)):\n",
        "    for i in range(len(condition)):\n",
        "        samples = sample_preperation(condition[i])\n",
        "        samples_all[condition[i]] = samples\n",
        "        model = lstm_decoder_encoder(samples)\n",
        "        # model = lstm_decoder_encoder_loung_attention(samples)\n",
        "        model, history = fit_model(model, samples)\n",
        "        model_all[condition[i]] = model\n",
        "        history_all[condition[i]] = history\n",
        "        results = model.evaluate(samples[\"X_input_test\"], samples[\"y_output_test\"], batch_size=64)\n",
        "        results_all[condition[i]] = results\n",
        "    # deepcopy to prevent resuing the same dictinary\n",
        "    samples_all_copy = deepcopy(samples_all)\n",
        "    model_all_copy = deepcopy(model_all)\n",
        "    history_all_copy = deepcopy(history_all)\n",
        "    results_all_copy = deepcopy(results_all)\n",
        "    # dictionary for everything\n",
        "    data_all[trial[j]][\"samples_all\"] = samples_all_copy\n",
        "    data_all[trial[j]][\"model_all\"] = model_all_copy\n",
        "    data_all[trial[j]][\"history_all\"] = history_all_copy\n",
        "    data_all[trial[j]][\"results_all\"] = results_all_copy\n",
        "\n",
        "# save dictionary as binary file\n",
        "data_all_file = open(\"data_all.pkl\",\"wb\")\n",
        "pkl.dump(data_all,data_all_file)\n",
        "data_all_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bgxvllcpe_m",
        "outputId": "751fa9ca-1d27-4af4-dffb-8880b4f2e1ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sit': <tensorflow.python.keras.engine.functional.Functional object at 0x0000028CA381F910>, 'gaze': <tensorflow.python.keras.engine.functional.Functional object at 0x0000028CADA67CD0>, 'gaze_swap': <tensorflow.python.keras.engine.functional.Functional object at 0x0000028CADE22E80>}\n",
            "{'sit': <tensorflow.python.keras.engine.functional.Functional object at 0x0000028CC5D93640>, 'gaze': <tensorflow.python.keras.engine.functional.Functional object at 0x0000028C9D3EDBE0>, 'gaze_swap': <tensorflow.python.keras.engine.functional.Functional object at 0x0000028C9B583A90>}\n"
          ]
        }
      ],
      "source": [
        "print(data_all[\"01\"][\"model_all\"])\n",
        "print(data_all[\"02\"][\"model_all\"])\n",
        "# print(data_all[\"03\"][\"model_all\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sit': [0.12468474358320236, 0.3171292245388031], 'gaze': [0.12614969909191132, 0.3186863958835602], 'gaze_swap': [0.12826953828334808, 0.32059457898139954]}\n",
            "{'sit': [0.12833917140960693, 0.3214663565158844], 'gaze': [0.1309717744588852, 0.32289862632751465], 'gaze_swap': [0.12642362713813782, 0.31922397017478943]}\n"
          ]
        }
      ],
      "source": [
        "for x in trial: print(data_all[x][\"results_all\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "02_model.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "140e3961e635afead8b973c7c4d4b2276a9d6240af442eb4cba65cde2b5b46ba"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('two-hearts': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
